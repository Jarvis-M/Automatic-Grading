# import asyncio
# import sys
# import os
# import logging

# # æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
# sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# from llm.deepseek_scorer import CodeContext
# from llm.llm_only_engine import LLMOnlyScoringEngine

# # è®¾ç½®æ—¥å¿—
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)

# async def grade_cpp_file_llm_only(problem_description: str, cpp_file_path: str,
#                                  compile_log: str = "", test_results: dict = None,
#                                  error_messages: list = None, warning_messages: list = None):
#     """
#     å¯¹CPPæ–‡ä»¶è¿›è¡Œçº¯LLMè¯„åˆ†

#     Args:
#         problem_description: é¢˜ç›®æè¿°
#         cpp_file_path: CPPæ–‡ä»¶è·¯å¾„
#         compile_log: ç¼–è¯‘æ—¥å¿—
#         test_results: æµ‹è¯•ç»“æœå­—å…¸ {æµ‹è¯•ç”¨ä¾‹å: æ˜¯å¦é€šè¿‡}
#         error_messages: é”™è¯¯ä¿¡æ¯åˆ—è¡¨
#         warning_messages: è­¦å‘Šä¿¡æ¯åˆ—è¡¨
#     """

#     try:
#         # è¯»å–CPPæ–‡ä»¶å†…å®¹
#         with open(cpp_file_path, 'r', encoding='utf-8') as f:
#             code_content = f.read()

#         # æ„å»ºä»£ç ä¸Šä¸‹æ–‡
#         context = CodeContext(
#             problem_description=problem_description,
#             recognized_code=code_content,
#             compile_log=compile_log or "",
#             test_results=test_results or {},
#             error_messages=error_messages or [],
#             warning_messages=warning_messages or []
#         )

#         # åˆå§‹åŒ–çº¯LLMè¯„åˆ†å¼•æ“
#         api_key = "sk-lszpP34FkMmgW6JyZURgKpRx9G6ld9zJJNTDiDHEpU9AMBzX"  # ä½ çš„APIå¯†é’¥
        
#         if not api_key:
#             return {
#                 "final_score": 0,
#                 "strategy": "error",
#                 "error": "APIå¯†é’¥æœªè®¾ç½®",
#                 "success": False
#             }

#         engine = LLMOnlyScoringEngine(api_key)
#         result = await engine.score(context)
#         return result

#     except FileNotFoundError:
#         logger.error(f"æ–‡ä»¶æœªæ‰¾åˆ°: {cpp_file_path}")
#         return {
#             "final_score": 0,
#             "strategy": "error",
#             "error": f"æ–‡ä»¶æœªæ‰¾åˆ°: {cpp_file_path}",
#             "success": False
#         }
#     except Exception as e:
#         logger.error(f"è¯„åˆ†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
#         return {
#             "final_score": 0,
#             "strategy": "error",
#             "error": str(e),
#             "success": False
#         }

# def print_llm_result(result):
#     """æ‰“å°LLMè¯„åˆ†ç»“æœ"""
#     print("=" * 60)
#     print("LLMè¯„åˆ†ç»“æœ:")
#     print("=" * 60)

#     if not result.get("success", False):
#         print(f"âŒ è¯„åˆ†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
#         return

#     print(f"âœ… æœ€ç»ˆåˆ†æ•°: {result['final_score']:.2f}")
#     print(f"ğŸ“‹ è¯„åˆ†ç­–ç•¥: {result['strategy']}")
#     print(f"ğŸ” éœ€è¦äººå·¥å¤æ ¸: {result['needs_manual_review']}")
    
#     if result['needs_manual_review']:
#         print(f"ğŸ“ å¤æ ¸åŸå› : {result['review_reason']}")

#     # æ‰“å°è¯¦ç»†è¯„åˆ†
#     if result.get('score_breakdown'):
#         breakdown = result['score_breakdown']
#         print("\nğŸ“Š è¯¦ç»†è¯„åˆ†:")
#         print("-" * 40)
        
#         scores = breakdown.get('scores', {})
#         print(f"â€¢ å¯ç¼–è¯‘æ€§: {scores.get('compilability', 0):.2f}/20")
#         print(f"â€¢ æ­£ç¡®æ€§: {scores.get('correctness', 0):.2f}/50")
#         print(f"â€¢ ä»£ç è´¨é‡: {scores.get('code_quality', 0):.2f}/20")
#         print(f"â€¢ å¯è¯»æ€§: {scores.get('readability', 0):.2f}/10")
#         print(f"â€¢ æ€»åˆ†: {breakdown.get('total', 0):.2f}/100")
#         print(f"â€¢ ç½®ä¿¡åº¦: {breakdown.get('confidence', 0):.2f}")

#         # æ˜¾ç¤ºè¯„åˆ†ç†ç”±
#         rationale = breakdown.get('rationale', '')
#         if rationale:
#             print(f"\nğŸ’¡ è¯„åˆ†ç†ç”±:")
#             print(f"  {rationale}")

#         # æ˜¾ç¤ºæ”¹è¿›å»ºè®®
#         suggestions = breakdown.get('suggestions', [])
#         if suggestions:
#             print(f"\nğŸ¯ æ”¹è¿›å»ºè®®:")
#             for i, suggestion in enumerate(suggestions, 1):
#                 print(f"  {i}. {suggestion}")

# async def main():
#     """ç¤ºä¾‹ç”¨æ³•"""
#     # ç¤ºä¾‹å‚æ•°
#     problem_desc = "ç¼–å†™ä¸€ä¸ªC++ç¨‹åºï¼Œè®¡ç®—ä¸¤ä¸ªæ•´æ•°çš„å’Œ"
#     cpp_file = r"E:\VScode\Python\recognition_c\Automatic-Grading\scoring\example.cpp"  # æ›¿æ¢ä¸ºä½ çš„CPPæ–‡ä»¶è·¯å¾„

#     # å¯é€‰å‚æ•°
#     # compile_log = "ç¼–è¯‘æˆåŠŸï¼Œæ— é”™è¯¯"
#     # error_messages = []
#     # warning_messages = ["å˜é‡å‘½åä¸è§„èŒƒ"]

#     print("ğŸš€ å¼€å§‹LLMè¯„åˆ†...")
    
#     try:
#         result = await grade_cpp_file_llm_only(
#             problem_desc, cpp_file
#             # , compile_log,
#             # test_results, error_messages, warning_messages
#         )

#         print_llm_result(result)

#     except Exception as e:
#         print(f"âŒ è¯„åˆ†å¤±è´¥: {e}")

# if __name__ == "__main__":
#     asyncio.run(main())

import asyncio
import sys
import os
import logging
import json
from typing import Dict, Any

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from llm.deepseek_scorer import CodeContext
from llm.llm_only_engine import LLMOnlyScoringEngine

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å®šä¹‰JSONæ–‡ä»¶ä¿å­˜è·¯å¾„
SCORING_RESULT_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), "scoring_result.json")

async def grade_cpp_file_llm_only(problem_description: str, cpp_file_path: str,
                                 compile_log: str = "", test_results: dict = None,
                                 error_messages: list = None, warning_messages: list = None) -> Dict[str, Any]:
    """
    å¯¹CPPæ–‡ä»¶è¿›è¡Œçº¯LLMè¯„åˆ†

    Args:
        problem_description: é¢˜ç›®æè¿°
        cpp_file_path: CPPæ–‡ä»¶è·¯å¾„
        compile_log: ç¼–è¯‘æ—¥å¿—
        test_results: æµ‹è¯•ç»“æœå­—å…¸ {æµ‹è¯•ç”¨ä¾‹å: æ˜¯å¦é€šè¿‡}
        error_messages: é”™è¯¯ä¿¡æ¯åˆ—è¡¨
        warning_messages: è­¦å‘Šä¿¡æ¯åˆ—è¡¨

    Returns:
        JSONæ ¼å¼çš„è¯„åˆ†ç»“æœ
    """

    try:
        # è¯»å–CPPæ–‡ä»¶å†…å®¹
        with open(cpp_file_path, 'r', encoding='utf-8') as f:
            code_content = f.read()

        # æ„å»ºä»£ç ä¸Šä¸‹æ–‡
        context = CodeContext(
            problem_description=problem_description,
            recognized_code=code_content,
            compile_log=compile_log or "",
            test_results=test_results or {},
            error_messages=error_messages or [],
            warning_messages=warning_messages or []
        )

        # åˆå§‹åŒ–çº¯LLMè¯„åˆ†å¼•æ“
        api_key = "sk-lszpP34FkMmgW6JyZURgKpRx9G6ld9zJJNTDiDHEpU9AMBzX"  # ä½ çš„APIå¯†é’¥
        
        if not api_key:
            error_result = {
                "success": False,
                "error": "APIå¯†é’¥æœªè®¾ç½®",
                "final_score": 0,
                "strategy": "error"
            }
            save_result_to_json(error_result)
            return error_result

        engine = LLMOnlyScoringEngine(api_key)
        result = await engine.score(context)
        print("11111")
        print(result)
        print("11111")
        # ä¿å­˜ç»“æœåˆ°æŒ‡å®šè·¯å¾„
        save_result_to_json(result)
        
        return result
    

    except FileNotFoundError:
        logger.error(f"æ–‡ä»¶æœªæ‰¾åˆ°: {cpp_file_path}")
        error_result = {
            "success": False,
            "error": f"æ–‡ä»¶æœªæ‰¾åˆ°: {cpp_file_path}",
            "final_score": 0,
            "strategy": "error"
        }
        save_result_to_json(error_result)
        return error_result
        
    except Exception as e:
        logger.error(f"è¯„åˆ†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        error_result = {
            "success": False,
            "error": str(e),
            "final_score": 0,
            "strategy": "error"
        }
        save_result_to_json(error_result)
        return error_result

def format_llm_result_for_api(result: Dict[str, Any]) -> Dict[str, Any]:
    """
    å°†LLMè¯„åˆ†ç»“æœæ ¼å¼åŒ–ä¸ºAPIå“åº”æ ¼å¼
    
    Args:
        result: LLMè¯„åˆ†ç»“æœ
        
    Returns:
        APIå“åº”æ ¼å¼çš„å­—å…¸
    """
    if not result.get("success", False):
        return {
            "success": False,
            "error": result.get("error", "æœªçŸ¥é”™è¯¯"),
            "final_score": 0
        }

    # æ„å»ºæ ‡å‡†åŒ–çš„APIå“åº”
    api_response = {
        "success": True,
        "final_score": result.get("final_score", 0),
        "strategy": result.get("strategy", "llm_only"),
        "needs_manual_review": result.get("needs_manual_review", False),
        "review_reason": result.get("review_reason", "")
    }

    # æ·»åŠ è¯¦ç»†çš„è¯„åˆ†åˆ†è§£
    if result.get('score_breakdown'):
        breakdown = result['score_breakdown']
        
        # åˆ†æ•°è¯¦æƒ…
        scores = breakdown.get('scores', {})
        score_details = {
            "compilability": scores.get('compilability', 0),
            "correctness": scores.get('correctness', 0),
            "code_quality": scores.get('code_quality', 0),
            "readability": scores.get('readability', 0),
            "total": breakdown.get('total', 0),
            "confidence": breakdown.get('confidence', 0)
        }
        
        api_response["score_breakdown"] = score_details
        
        # è¯„åˆ†ç†ç”±
        rationale = breakdown.get('rationale', '')
        if rationale:
            api_response["rationale"] = rationale
        
        # æ”¹è¿›å»ºè®®
        suggestions = breakdown.get('suggestions', [])
        if suggestions:
            api_response["suggestions"] = suggestions

    return api_response

def save_result_to_json(result: Dict[str, Any], filename: str = None):
    """
    å°†è¯„åˆ†ç»“æœä¿å­˜ä¸ºJSONæ–‡ä»¶åˆ°æŒ‡å®šè·¯å¾„
    
    Args:
        result: è¯„åˆ†ç»“æœ
        filename: ä¿å­˜çš„æ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
    """
    try:
        if filename is None:
            filepath = SCORING_RESULT_PATH
        else:
            # å¦‚æœæä¾›äº†ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
            if not os.path.isabs(filename):
                filepath = os.path.join(os.path.dirname(os.path.abspath(__file__)), filename)
            else:
                filepath = filename
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        logger.info(f"è¯„åˆ†ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        return filepath
    except Exception as e:
        logger.error(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")
        return None

def print_llm_result(result):
    """æ‰“å°LLMè¯„åˆ†ç»“æœï¼ˆç”¨äºå‘½ä»¤è¡Œè°ƒè¯•ï¼‰"""
    print("=" * 60)
    print("LLMè¯„åˆ†ç»“æœ:")
    print("=" * 60)

    if not result.get("success", False):
        print(f"âŒ è¯„åˆ†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        return

    print(f"âœ… æœ€ç»ˆåˆ†æ•°: {result['final_score']:.2f}")
    print(f"ğŸ“‹ è¯„åˆ†ç­–ç•¥: {result['strategy']}")
    print(f"ğŸ” éœ€è¦äººå·¥å¤æ ¸: {result['needs_manual_review']}")
    
    if result['needs_manual_review']:
        print(f"ğŸ“ å¤æ ¸åŸå› : {result['review_reason']}")

    # æ‰“å°è¯¦ç»†è¯„åˆ†
    if result.get('score_breakdown'):
        breakdown = result['score_breakdown']
        print("\nğŸ“Š è¯¦ç»†è¯„åˆ†:")
        print("-" * 40)
        
        scores = breakdown.get('scores', {})
        print(f"â€¢ å¯ç¼–è¯‘æ€§: {scores.get('compilability', 0):.2f}/20")
        print(f"â€¢ æ­£ç¡®æ€§: {scores.get('correctness', 0):.2f}/50")
        print(f"â€¢ ä»£ç è´¨é‡: {scores.get('code_quality', 0):.2f}/20")
        print(f"â€¢ å¯è¯»æ€§: {scores.get('readability', 0):.2f}/10")
        print(f"â€¢ æ€»åˆ†: {breakdown.get('total', 0):.2f}/100")
        print(f"â€¢ ç½®ä¿¡åº¦: {breakdown.get('confidence', 0):.2f}")

        # æ˜¾ç¤ºè¯„åˆ†ç†ç”±
        rationale = breakdown.get('rationale', '')
        if rationale:
            print(f"\nğŸ’¡ è¯„åˆ†ç†ç”±:")
            print(f"  {rationale}")

        # æ˜¾ç¤ºæ”¹è¿›å»ºè®®
        suggestions = breakdown.get('suggestions', [])
        if suggestions:
            print(f"\nğŸ¯ æ”¹è¿›å»ºè®®:")
            for i, suggestion in enumerate(suggestions, 1):
                print(f"  {i}. {suggestion}")

async def main():
    """ç¤ºä¾‹ç”¨æ³•ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    # ç¤ºä¾‹å‚æ•°
    problem_desc = "ç¼–å†™ä¸€ä¸ªC++ç¨‹åºï¼Œè®¡ç®—ä¸¤ä¸ªæ•´æ•°çš„å’Œ"
    cpp_file = r"E:\VScode\Python\recognition_c\Automatic-Grading\scoring\example.cpp"

    print("ğŸš€ å¼€å§‹LLMè¯„åˆ†...")
    
    try:
        # è·å–è¯„åˆ†ç»“æœ
        result = await grade_cpp_file_llm_only(problem_desc, cpp_file)
        
        # æ ¼å¼åŒ–ä¸ºAPIå“åº”æ ¼å¼
        api_response = format_llm_result_for_api(result)
        
        # æ‰“å°ç»“æœï¼ˆç”¨äºè°ƒè¯•ï¼‰
        print_llm_result(result)
        
        # æ‰“å°JSONæ ¼å¼ï¼ˆç”¨äºæŸ¥çœ‹APIå“åº”ï¼‰
        print("\nğŸ“„ JSONæ ¼å¼å“åº”:")
        print(json.dumps(api_response, ensure_ascii=False, indent=2))
        
        # æ‰“å°ä¿å­˜è·¯å¾„
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {SCORING_RESULT_PATH}")

    except Exception as e:
        print(f"âŒ è¯„åˆ†å¤±è´¥: {e}")

if __name__ == "__main__":
    asyncio.run(main())